#!/usr/bin/env python3
# Justin, 2022-12-20
"""Provides helper functions for data management."""

__all__ = ["pprint", "read_log"]

import datetime as dt
import json
import re
from typing import Optional, Type

import numpy as np
import tqdm

# For pprint to accept NaN values
NOVALUE = np.iinfo(np.int64).min


def pprint(
    *values,
    width: int = 7,
    out: Optional[str] = None,
    pbar: Optional[Type[tqdm.tqdm]] = None,
    stdout: bool = True,
):
    """Prints right-aligned columns of fixed width.

    Saved data is enforced to be tab-delimited to save storage space
    (typically not directly printed into console anyway, but post-processed).

    Args:
        out: Optional filepath to save data.
        pbar: tqdm.ProgressBar.
        print: Determines if should write to console.

    Note:
        The default column width of 7 is predicated on the fact that
        10 space-separated columns can be comfortably squeezed into a
        80-width terminal (with an extra buffer for newline depending
        on the shell).

        Not released.

        Integrates with tqdm.ProgressBar.

        Conflicts with Python's pprint module, which is implemented
        for pretty-printing of data structures instead of plain tabular data.
    """
    array = [(str(value) if value != NOVALUE else " ") for value in values]

    # Checks if progress bar is supplied - if so, update that instead
    if pbar:
        line = " ".join(array)
        pbar.set_description(line)

    # Prints line delimited to console
    elif stdout:
        line = " ".join([f"{value: >{width}s}" for value in array])
        print(line)

    # Write to file if filepath provided
    if out:
        line = "\t".join(array) + "\n"
        with open(out, "a") as f:
            f.write(line)

    return


def pprint_progressbar(value, lower=0, upper=1, width=80):
    """Prints multi-level progress bar for single valued representation.

    Visual representation of the magnitude of a single number is
    useful for fast optimization, as opposed to a numeric stream.

    TODO:
        Clean up code for reuse.
    """
    assert upper >= lower
    value = float(value)

    # Set lower and upper bounds
    bounds = [
        (0.5, 2.5),  # \u2588
        (0.2, 0.5),  # \u2593
        (0.1, 0.2),  # \u2592
        (0, 0.1),  # \u2591
    ]
    lefts = [0]
    for bound in bounds:
        lower, upper = bound
        percent = (value - lower) / (upper - lower)
        percent = max(0, min(1, percent))
        left = int(round(percent * (width - 7)))
        lefts.append(left)
    lefts.append(width - 7)

    # Print blocks
    blocks = [
        "\u2588",
        "\u2593",
        "\u2592",
        "\u2591",
        " ",
    ]
    line = "{:6.4f}\u2595".format(round(value, 4))
    for i in range(1, len(lefts)):
        amt = max(0, min(width - 7, lefts[i] - lefts[i - 1]))
        line += blocks[i - 1] * amt
    line += "\u258f"
    print(line)


def read_log(filename: str, schema: list, merge: bool = False):
    """Parses a logfile into a dictionary of columns.

    Convenience method to read out logfiles generated by the script.
    This is not filename-aware (i.e. date and schema version is not
    extracted from the filename) since these are not rigorously
    set-in-stone yet.

    If "time" is used, rows are assumed to be in chronological order,
    i.e. monotonically increasing, so timing overflows will be
    assumed to mean the next day. 'None' is also a valid datatype,
    i.e. ignored.

    Args:
        filename: Filename of log file.
        schema: List of datatypes to parse each column in logfile.
        merge:
            Whether multiple logging runs in the same file should
            be merged into a single list, or as a list-of-lists.

    Note:
        This code assumes tokens in columns do not contain spaces,
        including headers.

    TODO(Justin):
        Consider usage of PEP557 dataclasses for type annotations.
        Change the argument type of filename to include Path-like objects.
        Implement non-merge functionality.

        Not released.
    """

    convert_time_day_overflow = 0  # allow time conversions to cycle into the next day
    convert_time_prevtime = dt.datetime(1900, 1, 1, 0, 0, 0)

    def convert_time(s):
        nonlocal convert_time_prevtime
        nonlocal convert_time_day_overflow
        result = dt.datetime.strptime(s, "%H%M%S")  # default date is 1 Jan 1900
        if result < convert_time_prevtime:
            convert_time_day_overflow += 1
        convert_time_prevtime = result
        result += dt.timedelta(days=convert_time_day_overflow)
        return result

    def convert_datetime(s):
        return dt.datetime.strptime(s, "%Y%m%d_%H%M%S")

    # Parse schema
    _maps = []
    for dtype in schema:
        # Parse special (hardcoded) types
        if isinstance(dtype, str):
            if dtype == "time":
                _map = convert_time
            elif dtype == "datetime":
                _map = convert_datetime
            else:
                raise ValueError(f"Unrecognized schema value - '{dtype}'")
        elif dtype is None:  # ignore column
            _map = None
        # Treat everything else as regular Python datatypes
        elif isinstance(dtype, type):
            _map = dtype
        else:
            raise ValueError(f"Unrecognized schema value - '{dtype}'")
        _maps.append(_map)

    # Read file
    is_header_logged = False
    _headers = []
    _data = []
    print(_maps)
    with open(filename, "r") as f:
        for row_str in f:
            # Squash all intermediate spaces
            row = re.sub(r"\s+", " ", row_str.strip()).split(" ")
            try:
                # Equivalent to Pandas's 'applymap'
                # Note this cannot be run in parallel due to 'convert_time' implementation
                row = [f(v) for f, v in zip(_maps, row) if f is not None]
                _data.append(row)
            except:  # noqa: E722
                # If fails, assume is string header
                if not is_header_logged:
                    _headers = [v for f, v in zip(_maps, row) if f is not None]
                    is_header_logged = True

    if not is_header_logged:
        raise ValueError("Logfile does not contain a header.")

    # Merge headers
    _data = list(zip(*_data))
    _items = tuple(zip(_headers, _data))
    return dict(_items)


class DataEncoder(json.JSONEncoder):
    """Usage: json.dump(..., cls=data_encoder)"""

    @staticmethod
    def _dt2str(x):
        return x.strftime("%Y%m%d_%H%M%S.%f")

    def default(self, obj):
        if isinstance(obj, dt.datetime):
            return {"_dt": obj.strftime("%Y%m%d_%H%M%S.%f")}
        if isinstance(obj, np.ndarray):
            if len(obj) > 0 and isinstance(obj[0], dt.datetime):
                return {"_dt_np": list(map(DataEncoder._DT2STR, obj))}
            else:
                return {"_np": obj.tolist()}
        return super().default(obj)


def data_decoder(dct):
    """Usage: json.load(..., object_hook=datetime_decoder)"""

    def _str2dt(x):
        return dt.datetime.strptime(x, "%Y%m%d_%H%M%S.%f")

    if "_dt" in dct:
        return _str2dt(dct["_dt"])
    if "_np" in dct:
        return np.array(dct["_np"])
    if "_dt_np" in dct:
        return np.array(list(map(_str2dt, dct["_dt_np"])))
    return dct
